{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques 2: To get the top seller we did the following things\n",
    "\n",
    "* At first we created a dictionary where as key we stored the agent id and value for 0 (later to be filled by sale)\n",
    "* Then we merge each transaction csv with the productcsv based on 'product'\n",
    "* After that we group_by that newly created csv by 'agent' and get sum for the prices\n",
    "* Then we added the prices according to the agent key in dictionary from the newly created group_by dataframe.\n",
    "* Finally we sort the dictionary in descending order based on the prices to get top seller agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:21:17.037655Z",
     "iopub.status.busy": "2022-06-24T21:21:17.036495Z",
     "iopub.status.idle": "2022-06-24T21:21:17.315533Z",
     "shell.execute_reply": "2022-06-24T21:21:17.314601Z",
     "shell.execute_reply.started": "2022-06-24T21:21:17.037577Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Please change the path accordingly\n",
    "agent = pd.read_csv('../input/synaptican-dataset/agent.csv')\n",
    "product = pd.read_csv('../input/synaptican-dataset/product.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the product and price with transaction base on unique product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:21:21.060472Z",
     "iopub.status.busy": "2022-06-24T21:21:21.060134Z",
     "iopub.status.idle": "2022-06-24T21:21:26.057443Z",
     "shell.execute_reply": "2022-06-24T21:21:26.056457Z",
     "shell.execute_reply.started": "2022-06-24T21:21:21.060443Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(transaction, product[['product', 'price']], on=['product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:21:26.059762Z",
     "iopub.status.busy": "2022-06-24T21:21:26.059317Z",
     "iopub.status.idle": "2022-06-24T21:21:26.148840Z",
     "shell.execute_reply": "2022-06-24T21:21:26.147858Z",
     "shell.execute_reply.started": "2022-06-24T21:21:26.059726Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sale_dict = {}\n",
    "\n",
    "for each_agent in agent['agent']:\n",
    "    agent_sale_dict[each_agent] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:21:32.224794Z",
     "iopub.status.busy": "2022-06-24T21:21:32.224371Z",
     "iopub.status.idle": "2022-06-24T21:37:10.682816Z",
     "shell.execute_reply": "2022-06-24T21:37:10.681820Z",
     "shell.execute_reply.started": "2022-06-24T21:21:32.224762Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#agent_sale_dict = {}\n",
    "for i in range(20):\n",
    "    if i<10:\n",
    "        i = f'0{i}'\n",
    "    \n",
    "    #Change the path\n",
    "    transaction = pd.read_csv(f'../input/synaptican-dataset/transaction/part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')\n",
    "    df = pd.merge(transaction, product[['product', 'price']], on=['product'])\n",
    "    df = df.groupby(['agent']).sum().reset_index()\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(df)), desc=f\"Doint for {i} csv:\"):\n",
    "        agent_sale_dict[df.iloc[i]['agent']] += df.iloc[i]['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:38:49.858444Z",
     "iopub.status.busy": "2022-06-24T21:38:49.857408Z",
     "iopub.status.idle": "2022-06-24T21:38:50.053275Z",
     "shell.execute_reply": "2022-06-24T21:38:50.052249Z",
     "shell.execute_reply.started": "2022-06-24T21:38:49.858398Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sale_dict_sorted = dict(sorted(agent_sale_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the top seller agent and their sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:38:55.646487Z",
     "iopub.status.busy": "2022-06-24T21:38:55.646124Z",
     "iopub.status.idle": "2022-06-24T21:38:55.654239Z",
     "shell.execute_reply": "2022-06-24T21:38:55.652991Z",
     "shell.execute_reply.started": "2022-06-24T21:38:55.646456Z"
    }
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for k, v in agent_sale_dict_sorted.items():\n",
    "    if i==100:\n",
    "        break\n",
    "    print(k)\n",
    "    print(v)\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting isolated high seller agent who doesn't have any seller agent nearby within 500 meter\n",
    "\n",
    "* First we created a dictionary where key is the seller ID and value is a initialized with dummy list where we have (lat, lon, timestapmp) will be later filled\n",
    "* Then we iterate through the agent location and updated the dictionary based on the agent's latest location (the later dated location means the latest location)\n",
    "* The agent who doesn't have any location data is replaced by (0,0,datetime.datetime(1980,6,1)) data , basically on the equator\n",
    "* Then from the dictionary we created a agent location dataframe and sorted the dataframe based on latitude and longitude\n",
    "* Then we plot the agent location on the map and also the top selling agent in seperate map for visualization\n",
    "* Then we created a top_seller dataframe for the top 100 seller agent\n",
    "* We iterate through those agent and find their nearby agent and calculate the distace based on location\n",
    "* Those agent who did not have any agent nearby was appended to a list called 'final_list'\n",
    "* So we find out among the top 100 seller there are 40 agents who did not have any agent nearby 500 meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:39:05.615234Z",
     "iopub.status.busy": "2022-06-24T21:39:05.614484Z",
     "iopub.status.idle": "2022-06-24T21:39:05.935072Z",
     "shell.execute_reply": "2022-06-24T21:39:05.934100Z",
     "shell.execute_reply.started": "2022-06-24T21:39:05.615195Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tqdm\n",
    "from dateutil import parser\n",
    "import math\n",
    "agent_loc_ts = {}\n",
    "\n",
    "for single_agent in tqdm.tqdm(agent['agent'].unique()):\n",
    "    agent_loc_ts[single_agent] = [0,0,datetime.datetime(1980,6,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T21:39:06.132789Z",
     "iopub.status.busy": "2022-06-24T21:39:06.132191Z",
     "iopub.status.idle": "2022-06-24T21:42:19.761769Z",
     "shell.execute_reply": "2022-06-24T21:42:19.760714Z",
     "shell.execute_reply.started": "2022-06-24T21:39:06.132748Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(len(agent)), desc=\"Generating agent latest loc:\"):\n",
    "    date = parser.parse(agent.iloc[i]['ts'])\n",
    "    if agent_loc_ts[agent.iloc[i]['agent']][-1] < date and not math.isnan(agent.iloc[i]['lat']):\n",
    "        \n",
    "        agent_loc_ts[agent.iloc[i]['agent']][0] = agent.iloc[i]['lat']\n",
    "        agent_loc_ts[agent.iloc[i]['agent']][1] = agent.iloc[i]['lon']\n",
    "        agent_loc_ts[agent.iloc[i]['agent']][2] = date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a cleaned agent csv with only locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:00:54.631482Z",
     "iopub.status.busy": "2022-06-24T22:00:54.631130Z",
     "iopub.status.idle": "2022-06-24T22:00:55.484779Z",
     "shell.execute_reply": "2022-06-24T22:00:55.483786Z",
     "shell.execute_reply.started": "2022-06-24T22:00:54.631451Z"
    }
   },
   "outputs": [],
   "source": [
    "location_df = pd.DataFrame()\n",
    "\n",
    "location_df['agent'] = [i for i in agent_loc_ts.keys()]\n",
    "location_df['lat'] = [i[0] for i in agent_loc_ts.values()]\n",
    "location_df['lon'] = [i[1] for i in agent_loc_ts.values()]\n",
    "sale_price = []\n",
    "\n",
    "for each_agent in location_df['agent']:\n",
    "    sale_price.append(agent_sale_dict_sorted[each_agent])\n",
    "\n",
    "location_df['sales'] = sale_price\n",
    "\n",
    "location_df.to_csv('agent_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:01:38.722003Z",
     "iopub.status.busy": "2022-06-24T22:01:38.721646Z",
     "iopub.status.idle": "2022-06-24T22:01:38.744575Z",
     "shell.execute_reply": "2022-06-24T22:01:38.743609Z",
     "shell.execute_reply.started": "2022-06-24T22:01:38.721975Z"
    }
   },
   "outputs": [],
   "source": [
    "location_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting map plot of all agents (Question 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:01:47.462868Z",
     "iopub.status.busy": "2022-06-24T22:01:47.462263Z",
     "iopub.status.idle": "2022-06-24T22:01:52.671262Z",
     "shell.execute_reply": "2022-06-24T22:01:52.667688Z",
     "shell.execute_reply.started": "2022-06-24T22:01:47.462828Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    location_df,  # Our DataFrame\n",
    "    lat = \"lat\",\n",
    "    lon = \"lon\",\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"agent\"],  # what to display when hovering mouse over coordinate\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting map plot of top agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:15.189382Z",
     "iopub.status.busy": "2022-06-24T22:02:15.189015Z",
     "iopub.status.idle": "2022-06-24T22:02:15.252675Z",
     "shell.execute_reply": "2022-06-24T22:02:15.251737Z",
     "shell.execute_reply.started": "2022-06-24T22:02:15.189353Z"
    }
   },
   "outputs": [],
   "source": [
    "top_seller_df = pd.DataFrame()\n",
    "ta = []\n",
    "lat = []\n",
    "lon = []\n",
    "for i, target_agent in enumerate(agent_sale_dict_sorted.keys()):\n",
    "    if i==100:\n",
    "        break\n",
    "    ta.append(target_agent)\n",
    "    lat.append(agent_loc_ts[target_agent][0])\n",
    "    lon.append(agent_loc_ts[target_agent][1])\n",
    "\n",
    "\n",
    "top_seller_df['top_agent'] = ta\n",
    "top_seller_df['lat'] = lat\n",
    "top_seller_df['lon'] = lon\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    top_seller_df,  # Our DataFrame\n",
    "    lat = \"lat\",\n",
    "    lon = \"lon\",\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"top_agent\"],  # what to display when hovering mouse over coordinate\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:24.688612Z",
     "iopub.status.busy": "2022-06-24T22:02:24.688255Z",
     "iopub.status.idle": "2022-06-24T22:02:24.758248Z",
     "shell.execute_reply": "2022-06-24T22:02:24.757285Z",
     "shell.execute_reply.started": "2022-06-24T22:02:24.688562Z"
    }
   },
   "outputs": [],
   "source": [
    "location_df = location_df.sort_values(['lat','lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find nearest two agents within 500m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:28.696712Z",
     "iopub.status.busy": "2022-06-24T22:02:28.696142Z",
     "iopub.status.idle": "2022-06-24T22:02:29.791128Z",
     "shell.execute_reply": "2022-06-24T22:02:29.790064Z",
     "shell.execute_reply.started": "2022-06-24T22:02:28.696669Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "import numpy as np\n",
    "\n",
    "dist=0\n",
    "dist2=0\n",
    "\n",
    "\n",
    "agent_arr = location_df['agent'].values\n",
    "final_list = []\n",
    "lat_isolate = []\n",
    "lon_isolate = []\n",
    "\n",
    "for each_top_seller in tqdm.tqdm(top_seller_df['top_agent']):\n",
    "    \n",
    "    idx = np.where(agent_arr == each_top_seller)[0][0]\n",
    "    lat1 = location_df.iloc[idx]['lat']\n",
    "    lon1 = location_df.iloc[idx]['lon']\n",
    "    cord1 = (lat1, lon1)\n",
    "    if idx == 0:\n",
    "        lat2 = location_df.iloc[idx+1]['lat']\n",
    "        lon2 = location_df.iloc[idx+1]['lon']\n",
    "        cord2 = (lat2, lon2)\n",
    "        dist = geopy.distance.geodesic(cord1, cord2).m\n",
    "    elif idx == len(location_df):\n",
    "        lat2 = location_df.iloc[idx-1]['lat']\n",
    "        lon2 = location_df.iloc[idx-1]['lon']\n",
    "        cord2 = (lat2, lon2)\n",
    "        dist = geopy.distance.geodesic(cord1, cord2).m\n",
    "    else:\n",
    "        lat2 = location_df.iloc[idx-1]['lat']\n",
    "        lon2 = location_df.iloc[idx-1]['lon']\n",
    "        lat3 = location_df.iloc[idx+1]['lat']\n",
    "        lon3 = location_df.iloc[idx+1]['lon']\n",
    "        cord2 = (lat2, lon2)\n",
    "        cord3 = (lat3, lon3)\n",
    "        dist = geopy.distance.geodesic(cord1, cord2).km\n",
    "        dist2 = geopy.distance.geodesic(cord1, cord3).km\n",
    "    if dist > 0.5 and dist2 > 0.5:\n",
    "        #print(dist, dist2)\n",
    "        \n",
    "        final_list.append(each_top_seller)\n",
    "        lat_isolate.append(lat1)\n",
    "        lon_isolate.append(lon1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final_list length shows the number of isolated agents and final_list contain their agent id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:32.926055Z",
     "iopub.status.busy": "2022-06-24T22:02:32.925222Z",
     "iopub.status.idle": "2022-06-24T22:02:32.932451Z",
     "shell.execute_reply": "2022-06-24T22:02:32.931467Z",
     "shell.execute_reply.started": "2022-06-24T22:02:32.926018Z"
    }
   },
   "outputs": [],
   "source": [
    "len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:34.248655Z",
     "iopub.status.busy": "2022-06-24T22:02:34.247655Z",
     "iopub.status.idle": "2022-06-24T22:02:34.257587Z",
     "shell.execute_reply": "2022-06-24T22:02:34.256514Z",
     "shell.execute_reply.started": "2022-06-24T22:02:34.248588Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the isolated agents on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T22:02:39.739187Z",
     "iopub.status.busy": "2022-06-24T22:02:39.738832Z",
     "iopub.status.idle": "2022-06-24T22:02:39.801640Z",
     "shell.execute_reply": "2022-06-24T22:02:39.800619Z",
     "shell.execute_reply.started": "2022-06-24T22:02:39.739158Z"
    }
   },
   "outputs": [],
   "source": [
    "isolated_df = pd.DataFrame()\n",
    "\n",
    "isolated_df['agent'] = final_list\n",
    "isolated_df['lat'] = lat_isolate\n",
    "isolated_df['lon'] = lon_isolate\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    isolated_df,  # Our DataFrame\n",
    "    lat = \"lat\",\n",
    "    lon = \"lon\",\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"agent\"],  # what to display when hovering mouse over coordinate\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques 3: To find Geo-blocks with retail\n",
    "\n",
    "* At first we find the map of all agents\n",
    "* We delete all the outlier locations by sorting the locations\n",
    "* Then we draw a grid mesh on the map \n",
    "* In the grid, x axis and y axis values are calculated using geolocation distance calculator. Using the each grid size of 1.2*0.6 Km^2\n",
    "* After that we run a grid search on the grid mesh constructed with the geo block size\n",
    "* Each grid find atleast 1 retail is added lastly to find total geo-blocks with retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing the geoblocks with grid on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/agent-modified/agent_cleaned.csv\")\n",
    "\n",
    "#excluding the outliers outside Bangladesh\n",
    "df = df.loc[df['lon'] >= 88]\n",
    "df = df.loc[df['lon'] < 92.5]\n",
    "df = df.loc[df['lat'] >= 21]\n",
    "df = df.loc[df['lat'] < 27]\n",
    "\n",
    "#df.plot()  # plots all columns against index\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "maximum_lon = df['lon'].max()\n",
    "minimum_lon = df['lon'].min()\n",
    "\n",
    "maximum_lat = df['lat'].max()\n",
    "minimum_lat = df['lat'].min()\n",
    "\n",
    "#using calculator: https://www.meridianoutpost.com/resources/etools/calculators/calculator-latitude-longitude-distance.php?\n",
    "y_ticks = np.arange(minimum_lat, maximum_lat, 0.005)    #latitudes\n",
    "x_ticks = np.arange(minimum_lon, maximum_lon, 0.012)  #longitudes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['lon'], df['lat'])\n",
    "ax.set_yticks(y_ticks, minor=False)\n",
    "ax.yaxis.grid(True, which='major')\n",
    "\n",
    "ax.set_xticks(x_ticks, minor=False)\n",
    "ax.xaxis.grid(True, which='major')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "set_size(18,18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding total number of geo-blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def number_of_geoblocks(long, latit, df):\n",
    "    total_lon = len(long)\n",
    "    total_lat = len(latit)\n",
    "    \n",
    "    geo_block_retail = 0\n",
    "    \n",
    "    print(f'Total geo-block is {(total_lon+1)*(total_lat+1)}')\n",
    "    \n",
    "    for i in tqdm(range(len(x_ticks)-1)):\n",
    "        a = (df['lon'] < x_ticks[i+1]) & (df['lon'] >= x_ticks[i])\n",
    "        \n",
    "        for j in (range(len(y_ticks)-1)):\n",
    "            b = (df['lat'] < y_ticks[j+1]) & (df['lat'] >= y_ticks[j])  \n",
    "            result = a & b\n",
    "\n",
    "            if(result.any()):\n",
    "                geo_block_retail = geo_block_retail+1\n",
    "                \n",
    "        #print(f'Total retail geoblock is {geo_block_retail} and total')    \n",
    "    return geo_block_retail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number of geo block with retail\n",
    "\n",
    "print(number_of_geoblocks(x_ticks,y_ticks,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qus 4: Targeted 10 Retailer deployment geo blocks\n",
    "* We will first sort the dataframe based on total sales of each agent in decending order\n",
    "* Then will start checking from top selling agent that if a 3x3 neighbourhood area is free of any other retailer\n",
    "* If whole 3x3 neighborhood is empty of any retailer then we again check for that geohash's 3x3 neighborhood\n",
    "* If neighborhood of empty geohash is also empty then we can finally target that geo block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##partho if you can provide me the df with lon,lat,sales from above then remove this cell\n",
    "\n",
    "\n",
    "\n",
    "## data loading with total sales\n",
    "df_sale = pd.read_csv(\"./agent_cleaned.csv\")\n",
    "df_sale.head()\n",
    "\n",
    "\n",
    "final = df_sale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the sorted based on total sales. We will start with top seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on sales\n",
    "sorted_df = df.sort_values(by=['sales'], ascending=False)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = list(x_ticks)\n",
    "y_ticks = list(y_ticks)\n",
    "\n",
    "def visualize_in_geoblock(co_ord, df):   \n",
    "    ## it will visualize the targeted coordinates among the map\n",
    "    topten = pd.DataFrame(co_ord,columns =['lon','lat'])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df['lon'], df['lat'],c=df['sales'], s=15, cmap='viridis')   #cmap = 'RdBu',\n",
    "    ax.set_yticks(y_ticks, minor=False)\n",
    "    ax.yaxis.grid(True, which='major')\n",
    "    ax.set_xticks(x_ticks, minor=False)\n",
    "    ax.xaxis.grid(True, which='major')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    ax.scatter(topten['lon'], topten['lat'], s=80,edgecolors='red', color = 'yellow')   #cmap = 'RdBu',\n",
    "    set_size(18,18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_neighbour(lon, lat, df):\n",
    "    ## this will find the maximum salers grid then search for neighbourhood 3*3 kernel if there is not any nearest retailer. \n",
    "    ## if there is not retailer around them then we can put one in that empty geo-block\n",
    "    ## input: longitude, latitude and full df\n",
    "    ## output: list of desired possible positions of targeted geo-blocks\n",
    "    \n",
    "    global x_ticks,y_ticks\n",
    "    \n",
    "    index_lon = min(range(len(x_ticks)), key=lambda i: abs(x_ticks[i]-lon))   #find the nearest grid line\n",
    "    index_lat = min(range(len(y_ticks)), key=lambda i: abs(y_ticks[i]-lat))\n",
    " \n",
    "    ###############\n",
    "    ### a  b  c ###\n",
    "    ### d  *  e ###\n",
    "    ### f  g  h ###\n",
    "    ###############\n",
    "    \n",
    "    #for i in (range(len(x_ticks)-1)):\n",
    "    ax = (df['lon'] >= x_ticks[index_lon-1]) & (df['lon'] < x_ticks[index_lon])\n",
    "    ay = (df['lat'] >= y_ticks[index_lat+1]) & (df['lat'] < y_ticks[index_lat])\n",
    "    a = ax & ay\n",
    "    \n",
    "    bx = (df['lon'] >= x_ticks[index_lon]) & (df['lon'] < x_ticks[index_lon+1])\n",
    "    by = ay #(df['lat'] >= y_ticks[index_lat+1]) & (df['lat'] < y_ticks[index_lat])\n",
    "    b = bx & by\n",
    "    \n",
    "    cx = (df['lon'] >= x_ticks[index_lon+1]) & (df['lon'] < x_ticks[index_lon+2])\n",
    "    cy = ay  #(df['lat'] >= y_ticks[index_lat+1]) & (df['lat'] < y_ticks[index_lat])\n",
    "    c = cx & cy\n",
    "    \n",
    "    dx = ax\n",
    "    dy = (df['lat'] >= y_ticks[index_lat]) & (df['lat'] < y_ticks[index_lat-1])\n",
    "    d = dx & dy\n",
    "    \n",
    "    ex = cx\n",
    "    ey = dy\n",
    "    e = ex & ey\n",
    "    \n",
    "    fx = ax\n",
    "    fy = (df['lat'] >= y_ticks[index_lat-1]) & (df['lat'] < y_ticks[index_lat-2])\n",
    "    f = fx & fy\n",
    "    \n",
    "    gx = bx\n",
    "    gy = fy\n",
    "    g = gx & gy\n",
    "    \n",
    "    hx = cx\n",
    "    hy = fy\n",
    "    h = hx & hy\n",
    "    \n",
    "    empty_found = (~(a.any() | b.any() | c.any() |d.any() |e.any() |f.any() |g.any() |h.any()))\n",
    "    \n",
    "    return empty_found,a,b,c,d,e,f,g,h,index_lon,index_lat\n",
    "\n",
    "\n",
    "def selecting_neighbor(empt,a,b,c,d,e,f,g,h,df, index_lon, index_lat):\n",
    "    global x_ticks,y_ticks\n",
    "    \n",
    "    if(empt):\n",
    "        #for each geohash:\n",
    "        if(~a.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon-1],y_ticks[index_lat+1], df)  #find neighborhood 3*3 for this empty geohash\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon-1],y_ticks[index_lat+1]]\n",
    "        if(~b.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon],y_ticks[index_lat+1], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon],y_ticks[index_lat+1]]\n",
    "        if(~c.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon+1],y_ticks[index_lat+1], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon+1],y_ticks[index_lat+1]]\n",
    "        if(~d.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon-1],y_ticks[index_lat], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon-1],y_ticks[index_lat]]\n",
    "        if(~e.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon+1],y_ticks[index_lat], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon+1],y_ticks[index_lat]]\n",
    "        if(~f.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon-1],y_ticks[index_lat-1], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon-1],y_ticks[index_lat-1]]\n",
    "        if(~g.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon],y_ticks[index_lat-1], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon],y_ticks[index_lat-1]]\n",
    "        if(~h.any()):\n",
    "            y,_,_,_,_,_,_,_,_,_,_ = find_neighbour(x_ticks[index_lon+1],y_ticks[index_lat-1], df)\n",
    "            if(y):\n",
    "                co_ordinate = [x_ticks[index_lon+1],y_ticks[index_lat-1]]\n",
    "        \n",
    "        return co_ordinate\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the coordinates of targeted 10 geo blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "coordinates = []\n",
    "while(i<10):\n",
    "    element = sorted_df.iloc[i]\n",
    "    longi = element['lon']\n",
    "    latit = element['lat']\n",
    "    y,a,b,c,d,e,f,g,h,ind_lon, ind_lat = find_neighbour(longi, latit, df)\n",
    "    cc = selecting_neighbor(y,a,b,c,d,e,f,g,h,df,ind_lon,ind_lat)\n",
    "    if(cc!= None):\n",
    "        coordinates.append(cc)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize all 10 points in geo block map\n",
    "visualize_in_geoblock(coordinates, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qus 11: Next big billboard placement\n",
    "* We will find the highest number of agent from the geo blocks in task 3. Because most number of agent in a geo block denotes mostly used area for business.\n",
    "* We will register the geographical position of that geo block containing highest number of agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def number_of_agents_in_geoblocks(long, latit, df):\n",
    "    total_lon = len(long)\n",
    "    total_lat = len(latit)\n",
    "    total_agent_in_geoblock = 0\n",
    "    top_agents = 0\n",
    "    top_agents_pos = []    #will save highest number of agents in a geo block\n",
    "    \n",
    "    print(f'Total geo-block is {(total_lon+1)*(total_lat+1)}')\n",
    "    \n",
    "    for i in tqdm(range(len(x_ticks)-1)):\n",
    "        a = (df['lon'] < x_ticks[i+1]) & (df['lon'] >= x_ticks[i])\n",
    "        \n",
    "        for j in (range(len(y_ticks)-1)):\n",
    "            b = (df['lat'] < y_ticks[j+1]) & (df['lat'] >= y_ticks[j])  \n",
    "            result = a & b\n",
    "            total_agent_in_geoblock = sum(result)\n",
    "            if(total_agent_in_geoblock > top_agents):    # we will take the highest number of agents from the grid search in all geo blocks\n",
    "                top_agents = total_agent_in_geoblock\n",
    "                top_agents_pos = [x_ticks[i], y_ticks[j]]\n",
    "\n",
    "                    \n",
    "    return top_agents, top_agents_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, pos = number_of_agents_in_geoblocks(x_ticks,y_ticks,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize the mostly important point in geo block map\n",
    "visualize_in_geoblock(pos, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qus 5 & 9: Peak hour estimation\n",
    "\n",
    "# Our intuition is that the customer who did maximum transaction traveled most. Based on this intuition, we determined the top 20 customer with highest number transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data from kaggle\n",
    "!kaggle datasets download -d parthoghosh/synaptican-dataset\n",
    "!unzip \\*.zip  && rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data_list = os.listdir('/content/transaction') #listing all the csv file in the transaction directory\n",
    "dataset_list = sorted([os.path.join('transaction',i) for i in data_list]) #sorting alphanumerically (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell creates product-price dictionary pair where products are used as keys and prices are used as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv('product.csv') #loading product.csv as a pandas dataframe\n",
    "key_list = product_df['product'].values # listing all the product name\n",
    "value_list = product_df['price'].values #listing all the price\n",
    "zip_iterator = zip(key_list, value_list) # zipping the product-price pair\n",
    "price_dictionary = dict(zip_iterator) # converting them into a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell calculates the total sales and total transaction by hour for every csv file and append them to two separate list [sales list and transactions_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_list = [] #declaring sales list \n",
    "transaction_list = [] # declaring transaction list\n",
    "for dataset in dataset_list:  \n",
    "  df= pd.read_csv(dataset) #converting csv to dataframe\n",
    "  df['Hours'] = df['ts'].apply(lambda x : x.split(' ')[1].split(':')[0]) #getting the hour value from time stamp string\n",
    "  df = df.drop(['agent','customer','dt','ts'], axis =1) #droping unnecessary columns\n",
    "  df['Price'] = df['product'].apply(lambda x : price_dictionary[x]) #creating the price colmun according to the product\n",
    "  sales_list.append(df.groupby('Hours').sum()) #appends in to the sales list a dataframe containing total sales by hour\n",
    "  transaction_list.append(df['Hours'].value_counts()) # appends in to the transaction list a series object containing total number of product sold by hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We sum up all the hourly sales .The peak hour for sale is 19.00h as it can be seen after sorting in the output of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_sales = sales_list[0]\n",
    "for i in sales_list[1:]:\n",
    "  total_sales += i\n",
    "total_sales = total_sales.sort_values(by='Price',ascending=False)\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We sum up all the hourly transaction .The peak hour for number of transaction is 19.00h as it can be seen in the output of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transaction = transaction_list[0]\n",
    "for i in transaction_list[1:]:\n",
    "  total_transaction += i\n",
    "total_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import I\n",
    "customer_list = []\n",
    "for i in dataset_list :\n",
    "  print(i)\n",
    "  data = pd.read_csv(i)\n",
    "  for i in data['customer'].values :\n",
    "    customer_list.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "data = pd.read_csv(dataset_list[0])\n",
    "grouped_customer = data.groupby(\"customer\")\n",
    "grouped_agent = grouped_customer[\"agent\"].apply(list)\n",
    "grouped_agent = grouped_agent.reset_index()\n",
    "grouped_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = []\n",
    "for i in dataset_list :\n",
    "  print(i)\n",
    "  data = pd.read_csv(i)\n",
    "  for i in data['customer'].values :\n",
    "    customer_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = pd.DataFrame()\n",
    "\n",
    "custom_data['customer'] = customer_list\n",
    "\n",
    "custom_data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Retail Agents Serving highest number of travellers (Question No 10)\n",
    "\n",
    "* First read the transaction csv\n",
    "* Then find out the top 5 retail who has the most transactions\n",
    "* Then for those agent find out the unique customer they have served\n",
    "* Doing it for all the 20 transaction files and storing it in a dictionary where key is agent and value is the array of customer they have served\n",
    "* finally sorting that dictionary for value to find the best server retailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "agent = pd.read_csv('../input/synaptican-dataset/agent.csv')\n",
    "product = pd.read_csv('../input/synaptican-dataset/product.csv')\n",
    "#transaction = pd.read_csv('../input/synaptican-dataset/transaction/part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_serve_dict = {}\n",
    "\n",
    "for each_agent in agent['agent'].unique():\n",
    "    agent_serve_dict[each_agent] = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    if i<10:\n",
    "        i = f'0{i}'\n",
    "    \n",
    "    transaction = pd.read_csv(f'../input/synaptican-dataset/transaction/part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')\n",
    "    \n",
    "    df_high_count = transaction['agent'].value_counts().reset_index().head()\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(df_high_count)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    for each_agent in tqdm.tqdm(df_high_count['index']):\n",
    "        df_temp = transaction.loc[transaction['agent'] == each_agent]\n",
    "        arr = df_temp['customer'].unique()\n",
    "        agent_serve_dict[each_agent] = np.append(agent_serve_dict[each_agent], arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing out the best retailer (top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_agent in agent['agent'].unique():\n",
    "    if agent_serve_dict[each_agent].shape[0] != 0 and not isinstance(agent_serve_dict[each_agent], int):\n",
    "        agent_serve_dict[each_agent] = len(np.unique(agent_serve_dict[each_agent]))#len(set(agent_serve_dict[each_agent]))\n",
    "    else:\n",
    "        agent_serve_dict[each_agent] = 0\n",
    "\n",
    "agent_serve_dict = dict(sorted(agent_serve_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for i,(k,v) in enumerate(agent_serve_dict.items()):\n",
    "    print(k)\n",
    "    print(v)\n",
    "    if i==9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To forecast daily sales for the next week we did -\n",
    "\n",
    "* First we generate the total sale in each day of the year\n",
    "* The total dataset contains data of 175 days and we calculated the total sale in each and every day\n",
    "* Then we use the State-of-the-art forecasting model *Prophet* to forecast the sales for the furture days(next week)\n",
    "* We also use other visualization to demonstrate the trends of sales (like days and months sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agent = pd.read_csv('../input/synaptican-dataset/agent.csv')\n",
    "product = pd.read_csv('../input/synaptican-dataset/product.csv')\n",
    "#transaction = pd.read_csv('../input/synaptican-dataset/transaction/part-00000-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "dt = np.array([])\n",
    "price = np.array([])\n",
    "train_df = pd.DataFrame()\n",
    "for i in tqdm.tqdm(range(20)):\n",
    "    if i<10:\n",
    "        i = f'0{i}'\n",
    "    \n",
    "    transaction = pd.read_csv(f'../input/synaptican-dataset/transaction/part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')\n",
    "    df = pd.merge(transaction[['dt', 'product']], product[['product', 'price']], on=['product'])\n",
    "    df = df.groupby(['dt']).sum().reset_index()\n",
    "    dt = np.append(dt, df['dt'].values)\n",
    "    price = np.append(price, df['price'].values)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dt'] = dt\n",
    "train_df['price']=price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.groupby(['dt']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings; \n",
    "warnings.simplefilter('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pystan\n",
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the dataframe ready to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['Year'] = train_df['dt'].apply(lambda x: str(x)[:4])\n",
    "train_df['Month'] = train_df['dt'].apply(lambda x: str(x)[4:6])\n",
    "train_df['Day'] = train_df['dt'].apply(lambda x: str(x)[6:-2])\n",
    "\n",
    "train_df['ds'] = pd.DatetimeIndex(train_df['Year']+'-'+train_df['Month']+'-'+train_df['Day'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['dt','Year', 'Month', 'Day'], axis=1, inplace=True)\n",
    "train_df.columns = ['y', 'ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(interval_width=0.95, daily_seasonality=True)\n",
    "model = m.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "future = m.make_future_dataframe(periods=7,freq='D')\n",
    "forecast = m.predict(future)\n",
    "forecast.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1.savefig('forecast1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt2.savefig('forecast2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question (7 B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next dayâ€™s sales of each retail\n",
    "* There are more than 100k unique retail seller\n",
    "* so to forecast their sales we used the mathematical approach\n",
    "* we created a data frame and a csv file containing the agen id and their maximum sale price and minimum sale price and mean sale price\n",
    "* mean sale price will be that agent's next day's sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: If you face ram issue then restrat and run , each question section is a unique section not dependant on one other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agent = pd.read_csv('../input/synaptican-dataset/agent.csv')\n",
    "product = pd.read_csv('../input/synaptican-dataset/product.csv')\n",
    "#transaction = pd.read_csv('../input/synaptican-dataset/transaction/part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sales_dict = {}\n",
    "\n",
    "for each_agent in agent['agent'].unique():\n",
    "    agent_sales_dict[each_agent] = [0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "for i in range(20):\n",
    "    if i<10:\n",
    "        i = f'0{i}'\n",
    "    \n",
    "    transaction = pd.read_csv(f'../input/synaptican-dataset/transaction/part-000{i}-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv')\n",
    "    \n",
    "    df = pd.merge(transaction[['agent','dt', 'product']], product[['product', 'price']], on=['product'])\n",
    "    df_max = df.groupby(['agent']).max().reset_index()\n",
    "    df_min = df.groupby(['agent']).min().reset_index()\n",
    "    df_mean = df.groupby(['agent']).mean().reset_index()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(df_mean))):\n",
    "        l1 = agent_sales_dict[df_max.iloc[i]['agent']]\n",
    "        l2 = [df_max.iloc[i]['price'], df_min.iloc[i]['price'], df_mean.iloc[i]['price']]\n",
    "        agent_sales_dict[df_max.iloc[i]['agent']] = [sum(x) for x in zip(l1, l2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agent = list(agent_sales_dict.keys())\n",
    "all_mean = [i[2] for i in agent_sales_dict.values()]\n",
    "all_max = [i[0] for i in agent_sales_dict.values()]\n",
    "all_min = [i[1] for i in agent_sales_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the csv and the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sale_forecasting = pd.DataFrame()\n",
    "agent_sale_forecasting['agent'] = all_agent\n",
    "agent_sale_forecasting['max sale'] = all_max\n",
    "agent_sale_forecasting['min sale'] = all_min\n",
    "agent_sale_forecasting['mean sale'] = all_mean\n",
    "\n",
    "agent_sale_forecasting.to_csv('./agent_sale_forecasting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sale_forecasting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sale_forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 Area Categorization\n",
    "\n",
    "Here to solve the challenge, our intuition is that residential areas, transit hubs, and corporate areas show different transaction frequencies based on the time hour. On the other hand, tourist spots' traction varies with the month. Moreover, we have seen that transaction history with respect to time is almost similar for all 20 transaction csv. (The plot is shown in the presentation). That's why we demonstrated our algorithm for one transaction csv.\n",
    "\n",
    "First, we extracted the Tourist spots based on the moth analysis. Then we extracted the remaining places. Before doing that, we first clean the agent csv. (remove the outlrs and nan value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=pd.read_csv(\"agent_cleaned.csv\")\n",
    "agent=agent.loc[(agent['lat'] <= 26) & (21 <= agent['lat']) & (88 <= agent['lon']) & (agent['lon'] <= 92.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tourist spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('data/part-00001-afd80227-80b2-4b6a-aaaf-6e93851fc5cd-c000.csv') # Read one transaction csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Selct the common agents in agent and Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_org=set(agent['agent'].unique())\n",
    "agent_tran=set(Data['agent'].unique())\n",
    "\n",
    "agent_org_tran=agent_org.intersection(agent_tran)\n",
    "\n",
    "agent_org_tran=list(agent_org_tran)\n",
    "\n",
    "data=Data.loc[Data['agent'].isin(agent_org_tran)] # After agent filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['customer','product','dt'],axis=1) # drop customer, product,dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From time stand, extract only month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month']=[int(x.split(' ')[0].split('-')[1]) for x in data['ts']]\n",
    "data=data.drop(['ts'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agent = data.groupby(\"agent\")\n",
    "data_agent_month = data_agent[\"month\"].apply(list)\n",
    "data_agent_month = data_agent_month.reset_index()\n",
    "data_agent_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we calclulated the variance of the sell month. Intuitively, the tourist spots shuld have highest varinace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agent_month['var']=[np.var(x) for x in data_agent_month['month']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the agents which have highest varinace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tourist=data_agent_month[data_agent_month['var']==max(data_agent_month['var'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tourist=data_tourist.drop(['month','var'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tourist=agent.loc[agent['agent'].isin(data_tourist['agent'].values)].reset_index() # Agent tourist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_mapbox(\n",
    "    agent_tourist,  # your DataFrame which have lattitude and longitude\n",
    "    lat = \"lat\",  # lattitude column name\t\n",
    "    lon = \"lon\",  # longitude column name\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered (do not need to change)\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"agent\"],  # what to display when hovering mouse over coordinate\n",
    "    color='class',\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residential areas, Transit hubs, and Corporate areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_org=set(agent['agent'].unique())\n",
    "agent_tran=set(Data['agent'].unique())\n",
    "agent_tourist=set(agent_tourist['tourist'].values)\n",
    "\n",
    "agent_tran=agent_tran-agent_tourist # Remove the tourist agent\n",
    "\n",
    "agent_org_tran=agent_org.intersection(agent_tran)\n",
    "\n",
    "agent_org_tran=list(agent_org_tran)\n",
    "\n",
    "data=Data.loc[Data['agent'].isin(agent_org_tran)] # After agent filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['customer','product','dt'],axis=1) # drop customer, product,dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each agent, group all the transition hour time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour']=[int(x.split(' ')[1].split(':')[0]) for x in data['ts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hour=data.groupby(\"agent\")\n",
    "data_agent = data.groupby(\"agent\")\n",
    "data_agent_hour = data_agent[\"hour\"].apply(list)\n",
    "data_agent_hour = data_agent_hour.reset_index()\n",
    "data_agent_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the transaction time, we make 3 classes\n",
    "\n",
    "class=0 (Transit hubs)\n",
    "class=1 (Corporate areas)\n",
    "class=2 (Residential areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agent_hour['class'] = pd.Series(np.random.randn(len(data_agent_hour)), index=data_agent_hour.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,hl in enumerate (data_agent_hour['hour']):\n",
    "    \n",
    "    x=0\n",
    "    y=0\n",
    "    z=0\n",
    "    \n",
    "    for h in hl:\n",
    "        if (0<h<9):\n",
    "            x=x+1\n",
    "        elif (9<h<18):\n",
    "            y=y+1\n",
    "        else:\n",
    "            z=z+1\n",
    "            \n",
    "    if (x>y & x>z):\n",
    "        data_agent_hour['class'][ind]=0\n",
    "        \n",
    "    elif (y>x & y>z):\n",
    "        data_agent_hour['class'][ind]=1\n",
    "    else:\n",
    "        data_agent_hour['class'][ind]=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 3 datframes for 3 regions. \n",
    "\n",
    "agent_tran (Transit hubs)\n",
    "agent_cor (Corporate areas)\n",
    "agent_cor (Residential areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_trans=agent.loc[agent['agent'].isin(data_agent_hour[data_agent_hour['class']==0]['agent'].values)].reset_index() \n",
    "agent_cor=agent.loc[agent['agent'].isin(data_agent_hour[data_agent_hour['class']==1]['agent'].values)].reset_index() \n",
    "agent_res=agent.loc[agent['agent'].isin(data_agent_hour[data_agent_hour['class']==2]['agent'].values)].reset_index() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the agents location in map.\n",
    "\n",
    "For clarity, we ploted agent_res in seperate figure. To see the distincst location we ploted agent_trans and agent_cor in same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_trans_cop=pd.concat([agent_corp, agent_trans])\n",
    "agent_trans_cop # merge agent_corp, agent_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    agent_trans_cop,  # your DataFrame which have lattitude and longitude\n",
    "    lat = \"lat\",  # lattitude column name\t\n",
    "    lon = \"lon\",  # longitude column name\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered (do not need to change)\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"agent\"],  # what to display when hovering mouse over coordinate\n",
    "    color='class',\n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plot resedential areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    agent_res,  # your DataFrame which have lattitude and longitude\n",
    "    lat = \"lat\",  # lattitude column name\t\n",
    "    lon = \"lon\",  # longitude column name\n",
    "    center = {\"lat\": 24, \"lon\": 90},  # where map will be centered (do not need to change)\n",
    "    width = 600,  # Width of map\n",
    "    height = 600,  # Height of map\n",
    "    hover_data = [\"agent\"],  # what to display when hovering mouse over coordinate\n",
    "    \n",
    ")\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\") # adding beautiful street layout to map\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "The following portion of the notebook calculates total sales by hour per moth.\n",
    "\n",
    "Almost all the months shows similar kind of sale distribution over time .The month of january(month1) hase peak sales rate at 18h and for rest of the months peak sale is at 19h.This is because days are smaller in the month of january. From the graph we can see that high sale rate happens from 17th-20th hour when most of the offices are over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data_list = os.listdir('/content/transaction') #listing all the csv file in the transaction directory\n",
    "dataset_list = sorted([os.path.join('transaction',i) for i in data_list]) #sorting alphanumerically (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell creates product-price dictionary pair where products are used as keys and prices are used as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv('product.csv') #loading product.csv as a pandas dataframe\n",
    "key_list = product_df['product'].values # listing all the product name\n",
    "value_list = product_df['price'].values #listing all the price\n",
    "zip_iterator = zip(key_list, value_list) # zipping the product-price pair\n",
    "price_dictionary = dict(zip_iterator) # converting them into a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell calculates the total sales and total transaction by hour for every csv file and append them to two separate list [sales list and transactions_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_list = [] #declaring sales list \n",
    "transaction_list = [] # declaring transaction list\n",
    "months_sale = []\n",
    "for dataset in dataset_list: \n",
    "  print(dataset) \n",
    "  df= pd.read_csv(dataset) #converting csv to dataframe\n",
    "  df['Hours'] = df['ts'].apply(lambda x : x.split(' ')[1].split(':')[0]) #getting the hour value from time stamp string\n",
    "  df['Months'] = df['ts'].apply(lambda x : x.split('-')[1])\n",
    "  df = df.drop(['agent','customer','dt','ts'], axis =1) #droping unnecessary columns\n",
    "  df['Price'] = df['product'].apply(lambda x : price_dictionary[x]) #creating the price colmun according to the product\n",
    "  sales_list.append(df.groupby('Hours').sum()) #appends in to the sales list a dataframe containing total sales by hour\n",
    "  transaction_list.append(df['Hours'].value_counts()) # appends in to the transaction list a series object containing total number of product sold by hour\n",
    "  months_sale.append(df.groupby(['Months','Hours']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = months_sale[0]\n",
    "for i in months_sale[1:]:\n",
    "  total_sales += i\n",
    "#total_sales = total_sales.sort_values(by='Price',ascending=False)\n",
    "# total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month1 = total_sales[total_sales['Months']=='01']\n",
    "month2 =  total_sales[total_sales['Months']=='02']\n",
    "month3 =  total_sales[total_sales['Months']=='03']\n",
    "month4 =  total_sales[total_sales['Months']=='04']\n",
    "month5 =  total_sales[total_sales['Months']=='05']\n",
    "month6 =  total_sales[total_sales['Months']=='06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month1.plot.bar(x='Hours', y='Price', rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month2.plot.bar(x='Hours', y='Price', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month3.plot.bar(x='Hours', y='Price', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month4.plot.bar(x='Hours', y='Price', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month5.plot.bar(x='Hours', y='Price', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month6.plot.bar(x='Hours', y='Price', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : IN the following cells we calculate tatol number of interaction happend between a customer and different retailer \n",
    "Based on these interaction we can take a guess which customers travels the most.The higher the number of interaction , the more active customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = []\n",
    "for i in dataset_list :\n",
    "  print(i)\n",
    "  data = pd.read_csv(i)\n",
    "  for i in data['customer'].values :\n",
    "    customer_list.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = pd.DataFrame()\n",
    "\n",
    "custom_data['customer'] = customer_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_interaction_count = custom_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_interaction_count.head(20) #showing the top 20 active customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
